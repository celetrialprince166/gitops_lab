# =============================================================================
# Alertmanager Configuration â€” Notes Application
# =============================================================================
# File: monitoring/alertmanager.yml
# Deployed to: /opt/monitoring/alertmanager.yml on the Monitoring Server
#
# Receives firing alerts from Prometheus and routes them to Slack.
# The SLACK_WEBHOOK_URL placeholder is replaced at boot time by
# monitoring_user_data.sh with the actual webhook URL.
# =============================================================================

global:
  resolve_timeout: 5m # Mark alert resolved if not re-fired within 5 min

# =============================================================================
# Notification Templates
# =============================================================================
templates: []

# =============================================================================
# Routing Tree
# =============================================================================
route:
  # Group alerts by name + job so related events arrive in one message
  group_by: ["alertname", "job"]

  # Wait 30s after first alert before sending, to batch related alerts
  group_wait: 30s

  # Wait 5m before sending updates for an existing alert group
  group_interval: 5m

  # Don't re-notify for the same alert within 4 hours
  repeat_interval: 4h

  # Default receiver for all alerts
  receiver: slack-notifications

  # Route critical alerts to a dedicated channel/receiver
  routes:
    - match:
        severity: critical
      receiver: slack-critical
      repeat_interval: 1h # Repeat critical alerts more frequently

# =============================================================================
# Receivers
# =============================================================================
receivers:
  # â”€â”€ Slack â€” All Alerts (default) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  - name: slack-notifications
    slack_configs:
      - api_url: "SLACK_WEBHOOK_URL"
        channel: "#alerts"
        send_resolved: true
        title: >-
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}]
          {{ .CommonLabels.alertname }}
        text: >-
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Severity:* {{ .Labels.severity }}
          *Description:* {{ .Annotations.description }}
          {{ end }}

  # â”€â”€ Slack â€” Critical Alerts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  - name: slack-critical
    slack_configs:
      - api_url: "SLACK_WEBHOOK_URL"
        channel: "#alerts"
        send_resolved: true
        title: >-
          ðŸš¨ [{{ .Status | toUpper }}:{{ .Alerts.Firing | len }}]
          {{ .CommonLabels.alertname }}
        text: >-
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Severity:* `{{ .Labels.severity }}`
          *Instance:* {{ .Labels.instance }}
          *Description:* {{ .Annotations.description }}
          {{ end }}

# =============================================================================
# Inhibition Rules
# =============================================================================
# Suppress warning alerts when a critical alert is already firing for the
# same alertname and job. Prevents noise.
inhibit_rules:
  - source_match:
      severity: critical
    target_match:
      severity: warning
    equal: ["alertname", "job"]
